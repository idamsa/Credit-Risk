{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import math\n",
    "import seaborn as sns \n",
    "import sklearn\n",
    "from uszipcode import SearchEngine\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "credit = pd.read_csv(r\"C:\\Users\\Dell\\Documents\\Python Credit Risk\\Data\\SBAnational.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary \n",
    "credit.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type of data\n",
    "\n",
    "credit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(credit.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and feature engenieering\n",
    "\n",
    "# we use the information about the features in the pdf we received\n",
    "# they added some other features to the dataset like recession percentage of loan by SBA, loans backed by realestate which we could use too\n",
    "# also we will have to deal with all the missing values and impute them somehow or delete them\n",
    "# we will have to dummify the features like y/n to 0/1 and also the target to 0/1 meaning default/paied\n",
    "# we will have to deal with all the dates and use them to build the other column and discard them after\n",
    "# also city/ state/ zip describe same variable kinda same bank / bank state whci we will dleete because is says nothing about the customer\n",
    "# NAICS is type of business so we might need it\n",
    "# We have to fix the nas\n",
    "# We have to decide which features we keep as predctors and which we dont and if we need to build new features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check na\n",
    "credit.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the distribution of target feature\n",
    "Counter(credit['MIS_Status'])  # unbalanced + nas Counter({'P I F': 739609, 'CHGOFF': 157558, nan: 1997})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing state nas\n",
    "missing_rows = credit[credit['State'].isnull()].index\n",
    "search = SearchEngine()\n",
    "\n",
    "# impute State using zearch.by_zipcode function\n",
    "for i in missing_rows:\n",
    "    zipcode = search.by_zipcode(credit.iloc[i,1])\n",
    "    credit.iloc[i,0] = zipcode.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check how our NA's was imputed. We still have 2 NAs. One zipcode = 0 \n",
    "#and other is not in list the of search.by_zipcode function. We will remove them \n",
    "credit = credit.dropna(how='any', subset=['State'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have missing values in mis_status (our target variable). We will remove the rows in question because we cannot \n",
    "# impute them in a way that we are 100 % that it will be prepresentative , also they are nit that many\n",
    "credit = credit.dropna(how='any', subset=['MIS_Status'])\n",
    "# Change labels target feature\n",
    "credit.loc[credit['MIS_Status'] == \"P I F\", 'MIS_Status'] = 1  # Paid in full = 1\n",
    "credit.loc[credit['MIS_Status'] == \"CHGOFF\", 'MIS_Status'] = 0  # Charged off = 0 \n",
    "credit[\"MIS_Status\"] = credit.MIS_Status.astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nas is new exist and change to 0/1 label\n",
    "# after change type to object we will impute nas to most frequent value\n",
    "credit[\"NewExist\"] = credit.NewExist.astype(object)\n",
    "credit['NewExist'].fillna(1, inplace=True)  # fill nas with 1 (most frequent)\n",
    "credit.loc[credit['NewExist'] == 0, 'NewExist'] = 1 # change the 0 to 1 (most frequent)\n",
    "credit.loc[credit['NewExist'] == 2, 'NewExist'] = 0 # change 2 to 0 so 0 = new, 1 = established\n",
    "credit[\"NewExist\"] = credit.NewExist.astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column loans backed by “RealEstate,” where  “RealEstate” = 1 if “Term” > 240 months \n",
    "# and “RealEstate” = 0 if “Term” <240 months/ Counter({0: 831027, 1: 66138}) \n",
    "credit['RealEstate'] = np.where(credit['Term'] > 240, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix na and relabel LowDoc Loan Program: Y = Yes, N = No to 0= no and 1 = yes # we have also other values (S,A,0,R,C,1) that\n",
    "# we will also change to 1 or 0\n",
    "# We will fill nas with most frequent aka N (no)\n",
    "credit['LowDoc'].fillna(\"N\", inplace=True)\n",
    "credit['LowDoc'] = credit.LowDoc.replace(dict.fromkeys(['C','1','S','A','R','0'], 'N'))\n",
    "\n",
    "# Change the label to low doc from N and Y to 0 for no and 1 for yes\n",
    "credit.loc[credit['LowDoc'] == \"N\", 'LowDoc'] = 0\n",
    "credit.loc[credit['LowDoc'] == \"Y\", 'LowDoc'] = 1\n",
    "credit[\"LowDoc\"] = credit.LowDoc.astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a column with the pecentage of sba covered of the loan(ratio of the\n",
    "# amount of the loan SBA guarantees and the gross amount approved by the bank (SBA_Appv/GrAppv).)\n",
    "cols_to_change = ['SBA_Appv', 'GrAppv']\n",
    "\n",
    "for col in cols_to_change:\n",
    "    credit[col] = credit[col].str[1:]\n",
    "    credit[col] = credit[col].str.slice(0, -2)\n",
    "    credit[col] = credit[col].replace(',', '', regex=True)\n",
    "    credit[col] = credit[col].astype(float)\n",
    "\n",
    "credit['Portion'] = round((credit['SBA_Appv'] / credit['GrAppv']) *100)  #Build the new column\n",
    "credit[\"Portion\"] = credit.Portion.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAICS is the code for the industry\n",
    "# We can see we have the value 0 for 201666 of the rows\n",
    "\n",
    "# Let's check the mis status for those rows. If they are not many party of the minority class we could delete them\n",
    "NAICS_0 = credit[credit['NAICS'] == 0]\n",
    "Counter(NAICS_0.MIS_Status).values() # 184868 - 0, 16798 - 1, we can delete them\n",
    "credit = credit[credit.NAICS != 0]\n",
    "\n",
    "# We will leave just the two first numbers of the code that tells the industry(general) instead of the 5 numbers that are more in detail\n",
    "def first_two(d):                \n",
    "     return (d // 10 ** (int(math.log(d, 10)) - 1))\n",
    "credit[\"NAICS\"] = credit.NAICS.astype(int)\n",
    "credit['NAICS'] = credit.NAICS.apply(first_two)\n",
    "credit[\"NAICS\"] = credit.NAICS.astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(credit, kind=\"scatter\", hue=\"MIS_Status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After analyzing the plot above we will delete the NoEmp, Createjob, 'RetainedJob','FranchiseCode', 'UrbanRural', 'RevLineCr' bacause they say nothing about the label\n",
    "# The 'DisbursementGross', 'BalanceGross', 'ChgOffPrinGr' they are all 3 amounts that we can get only after the end of the credit period so we will not use it for the prediction\n",
    "# so we will delete them as they have no use in our predictions\n",
    "# We can delete the zip because we already used it to fill the state\n",
    "# we can delete the term because we used it for bulding the real estate column \n",
    "# we can delete all the dates because they have a lot of na and we will not use them in the prediction ( 'ChgOffDate', 'DisbursementDate')\n",
    "# We also delete sba app value because we used it to build the portion %\n",
    "# Name and id are unique we drop entire column.\n",
    "# we also delete the city (because we have state and zip and we dont want duplicated predictors), Bank Name (many values and not related to customer)\n",
    "\n",
    "# def delete_columns(df):\n",
    "#     df = df.drop(columns = [['NoEmp', 'CreateJob','RetainedJob','FranchiseCode','UrbanRural','RevLineCr','DisbursementGross', 'BalanceGross','ChgOffPrinGr','Zip', 'ChgOffDate','DisbursementDate','SBA_Appv','ApprovalDate','ApprovalFY','Term','LoanNr_ChkDgt', 'City','Name','Bank','BankState']])\n",
    "#     return df\n",
    "# delete_columns(credit)\n",
    "credit = credit.drop(columns=['LoanNr_ChkDgt', 'City','Name','Bank','BankState'])\n",
    "credit = credit.drop(columns=['Zip', 'ChgOffDate','DisbursementDate','SBA_Appv','ApprovalDate','ApprovalFY','Term'])\n",
    "credit = credit.drop(columns=['NoEmp', 'CreateJob','RetainedJob','FranchiseCode','UrbanRural','RevLineCr','DisbursementGross', 'BalanceGross','ChgOffPrinGr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check GrAppv(maybe we bin it) highly skewed to the left\n",
    "plt.figure(figsize = (15, 8))\n",
    "sns.distplot(credit.GrAppv, color = \"g\", kde = False)\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Approved ammount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning the gr app\n",
    "credit['GrAppv'] = pd.cut(credit['GrAppv'], 4)\n",
    "\n",
    "# we will also bin the portion column\n",
    "credit['Portion'] = pd.cut(credit['Portion'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed dataframe \n",
    "credit.to_csv('credit_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the label\n",
    "# Class imbalance we have many more of 1 (paid) than not paid loans. We will try fix this by downsampling the majority class\n",
    "credit['MIS_Status'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(credit['MIS_Status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the imbalance by downsampling the minority class\n",
    "\n",
    "# Separate majority and minority classes\n",
    "credit_majority = credit[credit.MIS_Status==1]\n",
    "credit_minority = credit[credit.MIS_Status==0]\n",
    " \n",
    "# Downsample majority class\n",
    "credit_majority_downsampled = resample(credit_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=140758,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "credit_downsampled = pd.concat([credit_majority_downsampled, credit_minority])\n",
    " \n",
    "# Display new class counts\n",
    "Counter(credit_downsampled['MIS_Status'])  # Counter({1: 140758, 0: 140758})\n",
    "\n",
    "credit_downsampled['MIS_Status'].value_counts().plot(kind='bar') # class imbalance fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot encoding categorical/ other values\n",
    "state_dummy = pd.get_dummies(credit_downsampled['State'])\n",
    "portion_dummy = pd.get_dummies(credit_downsampled['Portion'])\n",
    "grappv_dummy = pd.get_dummies(credit_downsampled['GrAppv'])\n",
    "naics_dummy = pd.get_dummies(credit_downsampled['NAICS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the dummyfied columns in order to append them after\n",
    "credit_downsampled = credit_downsampled.drop(columns=['State', 'Portion','GrAppv','NAICS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dummified data\n",
    "credit_dummy = pd.concat([credit_downsampled, naics_dummy], axis=1)\n",
    "credit_dummy = pd.concat([credit_dummy, grappv_dummy], axis=1)\n",
    "credit_dummy = pd.concat([credit_dummy, portion_dummy], axis=1)\n",
    "credit_dummy = pd.concat([credit_dummy, state_dummy], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save data\n",
    "# credit_dummy.to_csv('credit_downsampled_dummy.csv')\n",
    "\n",
    "# bring data\n",
    "credit_dummy =  pd.read_csv(r\"C:\\Users\\Dell\\Documents\\Python Credit Risk\\Data\\credit_downsampled_dummy.csv\", low_memory=False, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test and train datasets\n",
    "credit_dummy = credit_dummy.astype(int)\n",
    "y = credit_dummy.MIS_Status\n",
    "X = credit_dummy.drop(['MIS_Status'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------- RANDOM FOREST\n",
    "# OOB Random Forest Model \n",
    "# Check if exists already if not train\n",
    "try:\n",
    "    rf_OOB = pickle.load(open(r'C:\\Users\\Dell\\Documents\\Python Credit Risk\\Models\\rf_OOB.sav', 'rb'))\n",
    "except FileNotFoundError:\n",
    "    print(\"File doesen't exist, will train the model\")\n",
    "    # Build model\n",
    "    rf_OOB = RandomForestClassifier()\n",
    "    rf_OOB.fit(X_train,y_train)\n",
    "    # Save the model to disk\n",
    "    pickle.dump(rf_OOB, open(r'C:\\Users\\Dell\\Documents\\Python Credit Risk\\Models\\rf_OOB.sav', 'wb'))\n",
    "\n",
    "#  predict and performance\n",
    "rf_OOB_predict = rf_OOB.predict(X_test)\n",
    "rfc_cv_score = cross_val_score(rf_OOB, X, y, cv=10, scoring=\"roc_auc\")\n",
    "prob_rf_OOB = rf_OOB.predict_proba(X_test)\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, rf_OOB_predict))\n",
    "print('\\n')\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(y_test, rf_OOB_predict))\n",
    "print('\\n')\n",
    "print(\"=== All AUC Scores ===\")\n",
    "print(rfc_cv_score)\n",
    "print('\\n')\n",
    "print(\"=== Mean AUC Score ===\")\n",
    "print(\"Mean AUC Score - Random Forest: \", rfc_cv_score.mean())\n",
    "print(\"Accuracy for model: %.2f\" % (accuracy_score(y_test, rf_OOB_predict) * 100), ' % ')  #Accuracy for model: 64  %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "print (\"Features sorted by their score:\")\n",
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "for feature, importance in zip(X_test.columns, rf_OOB.feature_importances_):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
    "importances.sort_values(by='Gini-importance').plot(kind='bar', rot=45)\n",
    "pd.set_option('display.max_rows', 8)\n",
    "print(importances.sort_values(by='Gini-importance', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------- SVM\n",
    "# Train a linear SVM model using linear kernel\n",
    "svm_OOB_linear = sklearn.svm.LinearSVC(max_iter = 10000)\n",
    "svm_OOB_linear.fit(X_train, y_train)\n",
    "    \n",
    "# Make prediction\n",
    "svm_OOB_linear_pred = svm_OOB_linear.predict(X_test)\n",
    "    \n",
    "# Evaluate our model\n",
    "print(\"Evaluation linear kernel\")\n",
    "print(classification_report(y_test,svm_OOB_linear_pred))\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, svm_OOB_linear_pred)) # 63 % accuracy\n",
    "pickle.dump(svm_OOB_linear, open(r'C:\\Users\\Dell\\Documents\\Python Credit Risk\\Models\\svm_OOB_linear.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------- KNN\n",
    "knn_5 = KNeighborsClassifier(n_neighbors=5, n_jobs = 3)\n",
    "knn_5.fit(X_train, y_train)\n",
    "knn_5_pred = knn_5.predict(X_test)\n",
    "prob_knn_5 = knn_5.predict_proba(X_test)---------------------------------------------\n",
    "print(confusion_matrix(y_test, knn_5_pred ))\n",
    "print(classification_report(y_test, knn_5_pred ))\n",
    "print(\"Accuracy for model: %\" ,(accuracy_score(y_test, knn_5_pred ) * 100))\n",
    "pickle.dump(knn_5, open(r'C:\\Users\\Dell\\Documents\\Python Credit Risk\\Models\\knn_5.sav', 'wb'))\n",
    "# 60 % accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find best number neighbours\n",
    "error = []\n",
    "\n",
    "# Calculating error for K values between 90 and 101\n",
    "for i in range(90, 101):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error.append(np.mean(pred_i != y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the error for the number neighbours\n",
    "plt.figure(figsize=(90, 101))\n",
    "plt.plot(range(1, 100), error, color='red', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='blue', markersize=10)\n",
    "plt.title('Error Rate K Value')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Mean Error')\n",
    "\n",
    "# Best knn 97 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN 97 neighbours model\n",
    "knn_97 = KNeighborsClassifier(n_neighbors=97, n_jobs = 3)\n",
    "knn_97.fit(X_train, y_train)\n",
    "knn_97_pred = knn_97.predict(X_test)\n",
    "prob_knn = knn_97.predict_proba(X_test)\n",
    "print(confusion_matrix(y_test, knn_97_pred))\n",
    "print(classification_report(y_test, knn_97_pred))\n",
    "print(\"Accuracy for model: %\" ,(accuracy_score(y_test, knn_97_pred) * 100))\n",
    "# 63 % accuracy\n",
    "pickle.dump(knn_97, open(r'C:\\Users\\Dell\\Documents\\Python Credit Risk\\Models\\knn_97.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point we tried 3 models Random Forest, SVM and KNN with K = 97,they all have an accuracy close to 60 % minus the svm\n",
    "# We will try a model ensemble in order to better the predictions with knn and rf\n",
    "# Averaging\n",
    "final_pred_average = (prob_knn+prob_rf_OOB)/2\n",
    "final_pred_average_df = pd.DataFrame()\n",
    "final_pred_average_df[\"pred_0\"], final_pred_average_df[\"pred_1\"] = final_pred_average.T\n",
    "final_pred_average_df['Prediction'] = final_pred_average_df.idxmax(axis=1)\n",
    "\n",
    "dct = {\"pred_0\": 0 ,\n",
    "       'pred_1': 1 }\n",
    "\n",
    "final_pred_average_df = final_pred_average_df.assign(Prediction=final_pred_average_df.Prediction.map(dct))\n",
    "final_pred_average_df = final_pred_average_df.drop(columns=[\"pred_0\", \"pred_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compare the results of the average with the test values\n",
    "y_test_df = pd.DataFrame(y_test)\n",
    "y_test_df.index = np.arange(1, len(y_test_df) + 1)\n",
    "final_pred_average_df.index = np.arange(1, len(final_pred_average_df) + 1)\n",
    "final_pred_average_df = pd.concat([final_pred_average_df,y_test_df ], axis=1)\n",
    "final_pred_average_df = final_pred_average_df.astype(int)\n",
    "final_pred_average_df['result'] = np.where(final_pred_average_df['Prediction'] == final_pred_average_df['MIS_Status'], \"correct\", \"incorrect\")\n",
    "Counter(final_pred_average_df[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage(part, whole):\n",
    "  return 100 * float(part)/float(whole)\n",
    "\n",
    "percentage(36738,56304) # 65 % accuracy\n",
    "\n",
    "# While this is better than 1 algorithm at the time it seems that still we have problems \n",
    "# We will tru some other aproaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove features and train a random forest again. Features removed by importance. We will keep only the top 6 imp values\n",
    "credit_feature = credit_dummy[[\"(41.8, 61.2]\", \"RealEstate\", \"(61.2, 80.6]\",\"NewExist\",\"LowDoc\",\"62\", 'MIS_Status' ]]\n",
    "yf = credit_feature.MIS_Status\n",
    "Xf = credit_feature.drop(['MIS_Status'], axis=1)\n",
    "Xf_train, Xf_test, yf_train, yf_test = train_test_split(Xf, yf, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if exists already if not train\n",
    "try:\n",
    "    rf_features = pickle.load(open(r'C:\\Users\\Dell\\Documents\\Python Credit Risk\\Models\\rf_features.sav', 'rb'))\n",
    "except FileNotFoundError:\n",
    "    print(\"File doesen't exist, will train the model\")\n",
    "    # Build model\n",
    "    rf_features = RandomForestClassifier()\n",
    "    rf_features.fit(Xf_train,yf_train)\n",
    "    # Save the model to disk\n",
    "    pickle.dump(rf_features, open(r'C:\\Users\\Dell\\Documents\\Python Credit Risk\\Models\\rf_features.sav', 'wb'))\n",
    "\n",
    "#  predict and performance\n",
    "rf_features_predict = rf_features.predict(Xf_test)\n",
    "rfc_cv_score = cross_val_score(rf_features, Xf, yf, cv=10, scoring=\"roc_auc\")\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(yf_test, rf_features_predict))\n",
    "print('\\n')\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(yf_test, rf_features_predict))\n",
    "print('\\n')\n",
    "print(\"=== All AUC Scores ===\")\n",
    "print(rfc_cv_score)\n",
    "print('\\n')\n",
    "print(\"=== Mean AUC Score ===\")\n",
    "print(\"Mean AUC Score - Random Forest: \", rfc_cv_score.mean())\n",
    "print(\"Accuracy for model: %.2f\" % (accuracy_score(yf_test, rf_features_predict) * 100), ' % ')  #Accuracy for model: 62  %\n",
    "\n",
    "# This model has the same accuracy as the first one but trains much faster\n",
    "# will try to tune it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "# number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# number of features at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# max depth\n",
    "max_depth = [int(x) for x in np.linspace(100, 500, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# create random grid\n",
    "random_grid = {\n",
    " 'n_estimators': n_estimators,\n",
    " 'max_features': max_features,\n",
    " 'max_depth': max_depth\n",
    " }\n",
    "\n",
    "# Random search of parameters\n",
    "rfc_random = RandomizedSearchCV(estimator = rf_features, param_distributions = random_grid, n_iter = 3, cv = 3, verbose=2, random_state=42, n_jobs = 3)\n",
    "\n",
    "# Fit the model\n",
    "rfc_random.fit(Xf_train, yf_train)\n",
    "\n",
    "# print results\n",
    "print(rfc_random.best_params_)  #{'n_estimators': 1000, 'max_features': 'auto', 'max_depth': 140}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned model Random Forest\n",
    "# Check if exists already if not train\n",
    "try:\n",
    "    rf_tuned = pickle.load(open(r'C:\\Users\\Dell\\Documents\\Python Credit Risk\\Models\\rf_tuned.sav', 'rb'))\n",
    "except FileNotFoundError:\n",
    "    print(\"File doesen't exist, will train the model\")\n",
    "    # Build model with chosen hyperparamenters\n",
    "    rf_tuned = RandomForestClassifier(n_estimators=1000, max_depth=140, max_features='auto')\n",
    "    rf_tuned.fit(Xf_train,yf_train)\n",
    "    # Save the model to disk\n",
    "    pickle.dump(rf_tuned, open(r'C:\\Users\\Dell\\Documents\\Python Credit Risk\\Models\\rf_tuned.sav', 'wb'))\n",
    "\n",
    "#  predict and performance\n",
    "rf_tuned_predict = rf_tuned.predict(Xf_test)\n",
    "rf_tuned_cv_score = cross_val_score(rf_tuned, X, y, cv=10, scoring='roc_auc', n_jobs = 3)\n",
    "prob_rf_tuned = rf_tuned.predict_proba(Xf_test)\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(yf_test, rf_tuned_predict))\n",
    "print('\\n')\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(yf_test, rf_tuned_predict))\n",
    "print('\\n')\n",
    "print(\"=== All AUC Scores ===\")\n",
    "print(rf_tuned_cv_score)\n",
    "print('\\n')\n",
    "print(\"=== Mean AUC Score ===\")\n",
    "print(\"Mean AUC Score - Random Forest: \", rf_tuned_cv_score.mean())\n",
    "print(\"Accuracy for model: %.2f\" % (accuracy_score(yf_test, rf_tuned_predict) * 100))  # Accuracy for model: 62 % same perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XG BOOST\n",
    "\n",
    "# Fix the feature names so we can pass them to xgboost\n",
    "import re\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "credit_feature.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) \n",
    "                        else col for col in credit_feature.columns.values]\n",
    "yg = credit_feature.MIS_Status\n",
    "Xg = credit_feature.drop(['MIS_Status'], axis=1)\n",
    "Xg_train, Xg_test, yg_train, yg_test = train_test_split(Xg, yg, test_size=0.20)\n",
    "\n",
    "# fit model on original training data\n",
    "from xgboost import XGBClassifier\n",
    "try:\n",
    "    xgb = pickle.load(open(r'C:\\Users\\Dell\\Documents\\Python Credit Risk\\Models\\xgb.sav', 'rb'))\n",
    "except FileNotFoundError:\n",
    "    print(\"File doesen't exist, will train the model\")\n",
    "    # Build model\n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(Xg_train,yg_train)\n",
    "    # Save the model to disk\n",
    "    pickle.dump(xgb, open(r'C:\\Users\\Dell\\Documents\\Python Credit Risk\\Models\\rf_features.sav', 'wb'))\n",
    "\n",
    "#  predict and performance\n",
    "xgb_predict = xgb.predict(Xg_test)\n",
    "kfold = sklearn.model_selection.KFold(n_splits=5)\n",
    "results = cross_val_score(xgb, Xg, yg, cv=kfold)\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(yg_test, xgb_predict))\n",
    "print('\\n')\n",
    "print(\"=== All AUC Scores ===\")\n",
    "print(results)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(yg_test, xgb_predict) * 100.0)) \n",
    "# Accuracy: 61.66% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "print(classification_report(y_test, y_pred)) # 61 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ada Boost\n",
    "classifier = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=200\n",
    ")\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "cm= confusion_matrix(y_test, predictions)\n",
    "print(cm)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test, predictions) * 100.0)) # 63 %\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this poit it seem that all the models have more or less the same performance ~60/63 % \n",
    "# We will look into the balancing of classes as a reason in loss of information and poor performance\n",
    "# Will try to add more data \n",
    "credit=pd.read_csv(r\"C:\\Users\\Dell\\Documents\\Python Credit Risk\\Data\\credit_preprocessed.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dummy = pd.get_dummies(credit['State'])\n",
    "portion_dummy = pd.get_dummies(credit['Portion'])\n",
    "grappv_dummy = pd.get_dummies(credit['GrAppv'])\n",
    "naics_dummy = pd.get_dummies(credit['NAICS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit = pd.concat([credit, naics_dummy], axis=1)\n",
    "credit = pd.concat([credit, grappv_dummy], axis=1)\n",
    "credit = pd.concat([credit, portion_dummy], axis=1)\n",
    "credit = pd.concat([credit, state_dummy], axis=1)\n",
    "credit = credit.drop(columns=['State', 'Portion','GrAppv','NAICS'])\n",
    "# # save data\n",
    "# credit_down2.to_csv(\"downsample_clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 467. MiB for an array with shape (88, 695492) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-220fb4f9d50e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcredit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'MIS_Status'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClusterCentroids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mX_resampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_resampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_resampled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\lib\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0marrays_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mArraysTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         self.sampling_strategy_ = check_sampling_strategy(\n",
      "\u001b[1;32mc:\\users\\dell\\lib\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[1;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0maccept_sparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicate_one_vs_all\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinarize_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    745\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 747\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    748\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32mc:\\users\\dell\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1995\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1996\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.values_from_object\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mvalues\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5441\u001b[0m         \"\"\"\n\u001b[0;32m   5442\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AXIS_REVERSED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5445\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mas_array\u001b[1;34m(self, transpose, items)\u001b[0m\n\u001b[0;32m    820\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interleave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtranspose\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_interleave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    838\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"object\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[0mitemmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 467. MiB for an array with shape (88, 695492) and data type float64"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "y = credit.MIS_Status\n",
    "X = credit.drop(['MIS_Status'], axis=1)\n",
    "cc = ClusterCentroids(random_state=0)\n",
    "X_resampled, y_resampled = cc.fit_resample(X, y)\n",
    "print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
